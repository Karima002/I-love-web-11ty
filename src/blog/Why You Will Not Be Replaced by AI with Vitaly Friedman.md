### Quiet vs. Visible AI
- Most companies relentlessly rush towards AI features, trying to make sure that they aren’t left behind. So AI must be highly visible / present.


### Design patterns fo AI

#### Types of AI Scaffolding
- AI can go way beyond a textbox. AI
- products experiment in categories:
- Task-driven workflows (step-by-step)
- Canvas interfaces (infinite board)
- Flow builders (nodes, flowchart)
- Command line (terminal, code)
- Voice interfaces (look-up, tasks)
- Co-pilots (assistants)
- Agents (autonomous)
- Chat (open/structured)
- Search (keywords/AI)
- MCP (external tools): work available for other AI tools
- Integrations (Jira, Slack) 

#### Shopify release - feature
- voice chat & screen share
- Chat where you can talk with the AI bot. 
- AI should know where you are en guide you. 

#### Manipulating and AI
- Two levels of prompts: global promt and local prompt.
- manipulating AI: vizcom.ai
- Elicit > AI tool for research!!! It 
- Consensus.app: AI with a filter button
- Websets.eva.ai: breaks down the prompt, gives you data sorted in a tabel.
- /ELI5: explain like i'm five

### Task-Oriented Interfaces
- AI is being embedded into taskoriented UIs — from co-pilots in
side panels and infinite canvases to agentic UX and semantic grids

### Accessibility Challenges
- 01 — Can’t use disabled buttons
- 02 — Disrupting “busy” messages
- 03 — Navigating in responses is hard
- 04 — Keyboard nav works bottom up
- 05 — Tab through interaction controls
- 06 — No fast “Skip to chat” links
- 07 — AI/non-AI content conflicts
- 08 — Repeating focus jumps → skip links
- 09 — Messages not announced → audible
- 10 — No way to turn of feedback controls
- 11 — Verbose walls of text → TL;DR
- 12 — Long reasoning traces → collapse
- 13 — Citations are hover-only
- 14 — No edit mode for voice input
- 15 — No collapsing for past chats
- 16 — No summary for past chats
- 17 — No tweaking for verbosity
- 18 — No arrow keys supported
- 19 — No keyboard shortcuts


### Voice ai
- app.sesame.com


### AI model Collapse
- LLMs consume so much synthetic sludge that their outputs degrade,
their safety fails. Result: as errors compound, performance declines for models trained on AI content.


### Model Collapse: AI Eats Itself
- When AI learns from data generated by itself, output quality degrades with each iteration. It’s like we’re living in Plato’s cave, seeing shadows of shadows rather than reality itself.

### Summary
- 01 — LLMs have no understanding, just numerical data.
- 02 — Respond by choosing right next token, one at a time.
- 03 — No AI can magically clean up content/technical debt.
- 04 — Every word matters, affects outcome (butterfly effect).
- 05 — AI has system prompts, we define custom instructions.
- 06 —We influence AI responses with temperature, top_p.
- 07 — Context windows are limited, so AI forgets over time.
- 08 — Feedback loops are slow, need time to have an effect.
- 09 — AI output is always a compromise: speed vs. accuracy.
- 10 — AI will twist its words to elicit approval from users.